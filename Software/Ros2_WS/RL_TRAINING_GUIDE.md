# RL Training Guide - Tá»‘i Æ¯u HÃ³a Tham Sá»‘ UVC Robot

## Má»¥c ÄÃ­ch
Sá»­ dá»¥ng há»c tÄƒng cÆ°á»ng (Reinforcement Learning) Ä‘á»ƒ robot tá»± Ä‘á»™ng tÃ¬m cÃ¡c tham sá»‘ Ä‘iá»u khiá»ƒn UVC tá»‘i Æ°u sao cho robot cÃ³ thá»ƒ cÃ¢n báº±ng tá»‘t nháº¥t khi bá»‹ tilt/push.

## Kiáº¿n TrÃºc Há»‡ Thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RL Training Node (Python)                              â”‚
â”‚  - Policy Network: State(3) â†’ Action(7)                 â”‚
â”‚  - Reward: stability_score - fall_penalty               â”‚
â”‚  - Publish: /uvc_parameters [gain, scale_base, ...]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ Float64MultiArray
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  UVC Controller (C++)                                   â”‚
â”‚  - Subscribe: /uvc_parameters                           â”‚
â”‚  - Update: gain, scale_base, step_duration, ...         â”‚
â”‚  - Publish: /robot_orientation (IMU angles)             â”‚
â”‚  - Control: Joint positions at 20Hz                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ Vector3 (pitch, roll, tilt_mag)
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gazebo/Ignition Simulator                              â”‚
â”‚  - Physics: Humanoid robot balance                      â”‚
â”‚  - Feedback: IMU sensor data                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ACTION SPACE (7 Tham Sá»‘)

| Index | Tham Sá»‘ | Min | Max | MÃ´ Táº£ |
|-------|---------|-----|-----|-------|
| 0 | **gain** | 0.10 | 0.30 | Äá»™ lá»£i toÃ n cá»¥c, kiá»ƒm soÃ¡t pháº£n á»©ng tá»•ng quÃ¡t |
| 1 | **scale_base** | 0.10 | 0.20 | Há»‡ sá»‘ scale hÃ¬nh há»c (0.1 = 10%, 0.2 = 20%) |
| 2 | **step_duration** | 20.0 | 35.0 | Äá»™ dÃ i bÆ°á»›c chÃ¢n (cycle, má»—i cycle = 50ms) |
| 3 | **landing_phase** | 5.0 | 8.0 | Sá»‘ cycle Ä‘á»ƒ háº¡ chÃ¢n tá»« Ä‘á»™ cao sang sÃ n |
| 4 | **stance_width** | 30.0 | 45.0 | Khoáº£ng cÃ¡ch giá»¯a 2 chÃ¢n (mm) |
| 5 | **max_foot_lift** | 8.0 | 15.0 | Äá»™ cao nÃ¢ng chÃ¢n khi di chuyá»ƒn (mm) |
| 6 | **recovery_rate** | 0.05 | 0.20 | Tá»‘c Ä‘á»™ phá»¥c há»“i sau cÃ¡ch Ä‘á»™ng |

## STATE SPACE (3 Biáº¿n Tráº¡ng ThÃ¡i)

```
state = [pitch, roll, tilt_magnitude]
```
- **pitch**: GÃ³c quay trÆ°á»›c-sau (Ä‘á»™)
- **roll**: GÃ³c quay trÃ¡i-pháº£i (Ä‘á»™)
- **tilt_magnitude**: sqrt(pitchÂ² + rollÂ²) - Ä‘á»™ lá»›n tilt tá»•ng há»£p

## REWARD FUNCTION

```python
stability_score = 1.0 / (1.0 + 0.05 * tilt_mag)  # [0, 1]
fall_penalty = 1.0 if tilt_mag > 45.0 else 0.0  # Detect fall
reward = stability_score - 2.0 * fall_penalty
```

**Giáº£i thÃ­ch:**
- Tilt cÃ ng nhá» â†’ reward cÃ ng cao (â†’ 1.0)
- Tilt cÃ ng lá»›n â†’ reward cÃ ng tháº¥p (â†’ 0)
- **Náº¿u robot ngÃ£ (tilt > 45Â°)** â†’ reward = -1.0 (penalty)

## CÃ¡ch Cháº¡y

### 1. Terminal 1: Cháº¡y Gazebo Simulation
```bash
cd ~/Desktop/NCKH_2026/Software/Ros2_WS
source install/setup.bash
ros2 launch ros2_pkg humanoid_balance.launch.py
```

### 2. Terminal 2: Cháº¡y RL Training
```bash
cd ~/Desktop/NCKH_2026/Software/Ros2_WS
source install/setup.bash
./run_rl_training.sh
# Hoáº·c trá»±c tiáº¿p:
ros2 run ros2_pkg rl_training_node
```

## Output Monitoring

### RL Training Node Output
```
Ep    0 Step   0 | Gain: 0.180 | Scale: 0.121 | StepDur: 25.0 | Tilt: 0.5Â° | Reward: 0.971
Ep    0 Step  50 | Gain: 0.185 | Scale: 0.119 | StepDur: 24.8 | Tilt: 2.3Â° | Reward: 0.879
...
ğŸ‰ NEW BEST! Episode 45 | Reward: 42.35 | AvgTilt: 1.2Â° | MaxTilt: 5.3Â° | BestParams: G=0.190 S=0.128 SD=23.5 LP=6.2

Checkpoint saved at episode 100
  Best Params: Gain=0.190, Scale=0.128, StepDur=23.5, LandingPhase=6.2
```

### UVC Controller Output
```
[RL UPDATE] gain=0.190 scale=0.128 step_dur=23.5 land_phase=6.2 stance=35.0 fh_max=12.0 rr=0.100
[IMU RAW] pitch=-0.52Â° roll=1.24Â°
[AFTER THRESHOLD] pitch=-0.48Â° roll=1.18Â° | tilt_mag=1.25Â°
[GEOM-Y] roll=0.021 rad | scale_factor=0.128 | roll_scaled=0.0027 | ks=-0.0025 | dyi_new=-0.5
```

## CÃ¡c Giai Äoáº¡n Training

### Phase 1: Early Learning (Episodes 0-100)
- Robot há»c cÃ¡c hÃ nh Ä‘á»™ng cÆ¡ báº£n
- Reward dao Ä‘á»™ng lá»›n
- Tilt trung bÃ¬nh: 5-15Â°

### Phase 2: Improvement (Episodes 100-500)
- Policy á»•n Ä‘á»‹nh hÆ¡n
- Reward tÄƒng dáº§n
- Tilt trung bÃ¬nh: 2-5Â°

### Phase 3: Fine-tuning (Episodes 500+)
- Há»™i tá»¥ Ä‘áº¿n local optimum
- Reward á»•n Ä‘á»‹nh á»Ÿ cao
- Tilt trung bÃ¬nh: < 2Â°

## Khi NÃ o Dá»«ng Training?

1. **Reward Plateau**: Reward khÃ´ng tÄƒng trong 50-100 episode â†’ RL Ä‘Ã£ há»™i tá»¥
2. **Good Performance**: AvgTilt < 2Â° trong 20 episode liÃªn tiáº¿p
3. **Time Limit**: Sau 1000-2000 episode lÃ  Ä‘á»§ tá»‘t

## Sau Khi Training

### 1. Láº¥y Best Parameters
Xem log cuá»‘i cÃ¹ng:
```
Best Params: Gain=0.195, Scale=0.130, StepDur=22.5, LandingPhase=6.5
```

### 2. Update vÃ o config
Cáº­p nháº­t trong `launch/humanoid_balance.launch.py`:
```python
'gain': 0.195,
'scale_base': 0.130,
'step_duration': 22.5,
'landing_phase': 6.5,
```

### 3. Test Performance
Cháº¡y robot vá»›i parameters má»›i, quan sÃ¡t:
- Tilt angles trong simulation
- Foot response timing
- Balance stability

## Troubleshooting

### 1. Robot ngÃ£ liÃªn tá»¥c
- **NguyÃªn nhÃ¢n**: Learning rate quÃ¡ cao hoáº·c action bounds sai
- **CÃ¡ch sá»­a**: Giáº£m learning rate (0.001 â†’ 0.0005) hoáº·c Ä‘iá»u chá»‰nh action bounds

### 2. Reward khÃ´ng tÄƒng
- **NguyÃªn nhÃ¢n**: Reward function khÃ´ng phÃ¹ há»£p
- **CÃ¡ch sá»­a**: Thay Ä‘á»•i há»‡ sá»‘: `0.05 * tilt_mag` â†’ `0.1 * tilt_mag`

### 3. Robot cháº­m
- **NguyÃªn nhÃ¢n**: step_duration quÃ¡ dÃ i
- **CÃ¡ch sá»­a**: Giáº£m max value: 35.0 â†’ 30.0

## Hyperparameters CÃ³ Thá»ƒ Äiá»u Chá»‰nh (RL node)

```python
# Learning rate
self.optimizer = optim.Adam(self.policy.parameters(), lr=0.001)

# Reward coefficients
stability_score = 1.0 / (1.0 + 0.05 * tilt_mag)  # TÄƒng 0.05 â†’ 0.1 Ä‘á»ƒ strict hÆ¡n
fall_penalty = 1.0 if tilt_mag > 45.0 else 0.0   # Giáº£m 45.0 â†’ 30.0 Ä‘á»ƒ strict hÆ¡n

# Episode length
for step in range(500):  # TÄƒng lÃªn 1000 Ä‘á»ƒ test lÃ¢u hÆ¡n
```

## References

- **RL Algorithm**: Policy Gradient (REINFORCE variant)
- **Network**: 2-layer MLP (256 units each)
- **Distribution**: Gaussian (Normal distribution for continuous actions)
- **Loss**: Cross-entropy loss Ã— reward
